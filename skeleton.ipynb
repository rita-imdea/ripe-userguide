{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A) ACCESSING YOUR TOOLS \n",
    "\n",
    "##### I) CREATING YOUR OWN MEASUREMENTS \n",
    "1. The first step is to keep your API key in a place you can access\n",
    "2. Next select a set of probes you will you use as source probe. Its better to keep that information in a <strong>csv format</strong>.\n",
    "3. Finally select a set of destinations and place them in a csv file as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give the Atlas api key an easy to remember variable name \n",
    "ATLAS_API_KEY = \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting the source probes in a Python list\n",
    "with open(\"your_source_probes.csv\",\"r\") as probes:\n",
    "    probesReader= csv.reader(probes,delimiter=\";\")\n",
    "    next(probes)\n",
    "    probesList= []\n",
    "    for i in probesReader:\n",
    "        probesList.append(i[0])\n",
    "    probesStr = \",\".join(probesList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting  the destination websites you wish to access in a python list\n",
    "with open(\"your_destination_websites.csv\",\"r\") as urls:\n",
    "    urlsReader = csv.reader(urls,delimiter=\"\\n\")\n",
    "    urlsList = []\n",
    "    for i in urlsReader:\n",
    "        urlsList.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating the measurements - this is where you first use your API key \n",
    "measurement_dict = {}\n",
    "for i in urlsList:\n",
    "    ping = Ping(\n",
    "                    af=4,\n",
    "                    target=i,\n",
    "                    interval=300, #run ping every 5 minutes \n",
    "                    description=\"Ping Test\"\n",
    "                )\n",
    "    traceroute = Traceroute(\n",
    "                                af=4,\n",
    "                                target=i,\n",
    "                                interval=7200,#run traceroute every 2 hours\n",
    "                                description=\"Traceroute Test\",\n",
    "                                protocol=\"ICMP\",\n",
    "                            )\n",
    "    source = AtlasSource(\n",
    "                                type=\"probes\",\n",
    "                                requested=len(probesList),\n",
    "                                value= probesStr,\n",
    "                                tags={\"exclude\": [\"system-anchor\"]}\n",
    "                            )\n",
    "    \n",
    "    atlas_request = AtlasCreateRequest(\n",
    "                                                start_time=datetime.utcnow()+ timedelta(seconds=300),\n",
    "                                                stop_time= datetime.utcnow()+ timedelta(seconds=300)+timedelta(days=2),\n",
    "                                                key=ATLAS_API_KEY,\n",
    "                                                measurements=[ping, traceroute],\n",
    "                                                sources=[source, source]\n",
    "                                        )\n",
    "\n",
    " (is_success, response) = atlas_request.create()\n",
    "    if is_success:\n",
    "        measurement_dict.update(response) \n",
    "    else:\n",
    "        print(response)\n",
    "    print(measurement_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. When you complete this part you will be given measurement IDs.\n",
    "5. Keep them because we are going to use them to  retrieve the measurements\n",
    "6. Accessing public measurements and accessing your own measurements follows the same procedure so we shall go to the next part of this session "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### II) ACCESSING PUBLICLY AVAILABLE MEASUREMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the measurement IDs you want to retrieve\n",
    "measurement_ids = [\" \"]\n",
    "\n",
    "# Loop through the measurement IDs and retrieve the JSON files\n",
    "for measurement_id in measurement_ids:\n",
    "        url = f\"https://atlas.ripe.net/api/v2/measurements/{measurement_id}/results/?format=json\"\n",
    "        headers = {\"Authorization\": f\"Bearer {ATLAS_API_KEY}\"}\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "                json_data = response.json()\n",
    "                \n",
    "                # Write the JSON data to a file\n",
    "                with open(f\"{measurement_id}.json\", \"w\") as f:\n",
    "                        json.dump(json_data, f, indent=4)\n",
    "                        \n",
    "        else:\n",
    "                print(f\"Failed to retrieve measurement {measurement_id}. Error code: {response.status_code}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Python has a library called Pandas which provides a data structure called a [Data frame] (https://www.w3schools.com/python/pandas/pandas_dataframes.asp).\n",
    "8. We shall use this to parse our data since the library provides a number of useful functions and the dataframe is easy to visualise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B) PARSING YOUR MEASUREMENTS\n",
    "\n",
    "1. In this step you combine your measurement files into a pandas dataframe \n",
    "2. Then you clean the data and remove the null values  \n",
    "3. Finally you can plot the probability distribution to see how the data is spread "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining your measurement files \n",
    "# We first import the necessary libraries \n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import json\n",
    "\n",
    "\n",
    "# Loop through the measurement IDs experiment files and create a DataFrame for each\n",
    "dfs = []\n",
    "for measurement_id in measurement_ids:\n",
    "    # Read the JSON data from the file\n",
    "    with open(f\"{measurement_id}.json\", \"r\") as f:\n",
    "        json_data = json.load(f)\n",
    "    \n",
    "    # Normalize the JSON data into a pandas DataFrame\n",
    "    df = pd.json_normalize(json_data)\n",
    "    \n",
    "    # Append the DataFrame to the list of DataFrames\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all the DataFrames into a single one\n",
    "result_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data \n",
    "\n",
    "# Change them to NaN\n",
    "result_df['avg'].replace(-1.0, np.nan, inplace=True)\n",
    "\n",
    "# Remove Null values \n",
    "result_df = result_df.dropna(how='any',axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the probe_id column for easy plotting \n",
    "nprb_id = []\n",
    "\n",
    "for value in df[\"prb_id\"]:\n",
    "    if value == 1004991:\n",
    "        nprb_id.append('es1')\n",
    "   \n",
    "        \n",
    "        \n",
    "result_df[\"nprb_id\"] = nprb_id\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the time column from epoch to date time format for time series processing \n",
    "new_timestamp = []\n",
    "\n",
    "for i in result_df['timestamp']:\n",
    "    my_datetime = datetime.fromtimestamp(i)\n",
    "    new_timestamp.append(my_datetime)\n",
    "\n",
    "df = result_df.copy()\n",
    "df['new_time'] = new_timestamp\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_name = []\n",
    "\n",
    "for value in df[\"nprb_id\"]:\n",
    "    if (value == 'es1') or (value == 'es2') or(value == 'es3') or( value == 'es4'):\n",
    "        country_name.append('Spain')\n",
    "    if(value == 'nl1') or (value == 'nl2')or (value == 'nl3'):\n",
    "        country_name.append('Netherlands')\n",
    "    if(value == 'pt1') or (value == 'pt2'):\n",
    "        country_name.append('Portugal')\n",
    "    if(value == 'us1') or (value == 'us2'):\n",
    "        country_name.append('USA')\n",
    "    if(value == 'ug1'):\n",
    "        country_name.append('Uganda')\n",
    "        \n",
    "df['country_name'] = country_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the probability distribution\n",
    "# checking cdf for each of the probes\n",
    "probes = ['es1','es2','es3','es4','nl1','nl3','pt2','ug1','us1','us2']\n",
    "\n",
    "for probe in probes:\n",
    "    df_cdf = df[(df['nprb_id'] == probe)]\n",
    "    axx = df_cdf['avg'].hist(cumulative=True, density=True, bins=100, alpha = 0.3)\n",
    "axx.legend(probes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking pdf for each of the probes\n",
    "countries = df['country_name'].unique()\n",
    "\n",
    "for country in countries:\n",
    "    df_pdf = df[(df['country_name'] == country)]\n",
    "    axx = df_pdf['avg'].hist(density=True, bins=100, alpha = 0.3)\n",
    "\n",
    "axx.legend(countries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C) ANALYZING YOUR MEASUREMENTS \n",
    "\n",
    "1. First you do some feature engineering ie add features that may be missing but could be important like distance and probe status \n",
    "2. Check how latency varies over time, how mean and standard deviation vary over distance or any other interesting scenarios you can come up with \n",
    "3. Finally you can do some predictions based using established mathematical models or machine learning models and see what gives you best results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering \n",
    "# Collect the source probe information \n",
    "\n",
    "from ripe.atlas.cousteau import Probe  \n",
    "\n",
    "probe_id_list = []\n",
    "\n",
    "#extract the probe cordinates from ripe atlas \n",
    "probe_coordinates = []\n",
    "probe_country = []\n",
    "\n",
    "for id_i in probe_id_list:\n",
    "    probe = Probe(id=id_i) # Obtains all metadata of probe id_i\n",
    "    print(probe.geometry) #probe.geometry is a GeoJSON https://en.wikipedia.org/wiki/GeoJSON\n",
    "    probe_coordinates.append(probe.geometry['coordinates']) # saving to the list\n",
    "    probe_country.append(probe.country_code)\n",
    "\n",
    "longitude = []\n",
    "latitude = []\n",
    "\n",
    "for i in probe_coordinates:\n",
    "    longitude.append(i[0])\n",
    "    latitude.append(i[1])\n",
    "\n",
    "# create a probe metadata dataframe\n",
    "srcprobes_df = pd.DataFrame({'prb_id': probe_id_list,'longitude': longitude, 'latitude': latitude,'probe_country': probe_country})\n",
    "srcprobes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the destination probe information \n",
    "\n",
    "probe_id_list = []\n",
    "\n",
    "#extract the probe cordinates from ripe atlas \n",
    "probe_coordinates = []\n",
    "probe_country = []\n",
    "\n",
    "for id_i in probe_id_list:\n",
    "    probe = Probe(id=id_i) \n",
    "    print(probe.geometry) \n",
    "    probe_coordinates.append(probe.geometry['coordinates']) # saving to the list\n",
    "    probe_country.append(probe.country_code)\n",
    "\n",
    "longitude = []\n",
    "latitude = []\n",
    "\n",
    "for i in probe_coordinates:\n",
    "    longitude.append(i[0])\n",
    "    latitude.append(i[1])\n",
    "\n",
    "# create a probe metadata dataframe\n",
    "dstprobes_df = pd.DataFrame({'prb_id': probe_id_list,'longitude': longitude, 'latitude': latitude,'probe_country': probe_country})\n",
    "dstprobes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the distances between all possible source probe and destination probe pairs \n",
    "from geopy.distance import distance\n",
    "from itertools import product\n",
    "\n",
    "# Create an empty list to store the distances\n",
    "data = []\n",
    "\n",
    "# Iterate over each combination of source and destination probes\n",
    "for source_row, dest_row in product(srcprobes_df.iterrows(), dstprobes_df.iterrows()):\n",
    "    source_row = source_row[1]  # Get the row data from the iterator\n",
    "    dest_row = dest_row[1]  # Get the row data from the iterator\n",
    "    \n",
    "    source_coordinates = (source_row['latitude'], source_row['longitude'])\n",
    "    dest_coordinates = (dest_row['latitude'], dest_row['longitude'])\n",
    "    \n",
    "    distance_km = distance(source_coordinates, dest_coordinates).km\n",
    "    # Append data to the list\n",
    "    data.append({\n",
    "        'source_prb_id': source_row['prb_id'],\n",
    "        'source_longitude': source_row['longitude'],\n",
    "        'source_latitude': source_row['latitude'],\n",
    "        'destination_prb_id': dest_row['prb_id'],\n",
    "        'destination_longitude': dest_row['longitude'],\n",
    "        'destination_latitude': dest_row['latitude'],\n",
    "        'distance': distance_km\n",
    "    })\n",
    "\n",
    "# Create a new dataframe from the data list\n",
    "distance_df = pd.DataFrame(data)\n",
    "\n",
    "# Print the new dataframe\n",
    "distance_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a map from the distance dataframe and and use it to map distances to all probes and all destinations \n",
    "\n",
    "# Create a dictionary mapping (source_prb_id, destination_prb_id) to distance\n",
    "distance_map = {(int(row['source_prb_id']), int(row['destination_prb_id'])): row['distance'] for _, row in distance_df.iterrows()}\n",
    "\n",
    "\n",
    "# Map the distance values to the existing DataFrame based on (source_prb_id, destination_prb_id)\n",
    "df['distance'] = df.apply(lambda row: distance_map.get((int(row['prb_id']), int(row['dst_id']))), axis=1)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "distance_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the probe status \n",
    "# you have to download the connection log json files manually \n",
    "\n",
    "probe_id_list = []\n",
    "\n",
    "# Dictionary to store combined json files with probe status \n",
    "combined_dict = []\n",
    "\n",
    "# Iterate over servers\n",
    "for server in probe_id_list:\n",
    "    server_name = str(server)\n",
    "    \n",
    "    # Read JSON log file\n",
    "    log_file = server_name + '.json'\n",
    "    \n",
    "    results = process_data(log_file, server)\n",
    "        \n",
    "    # Append to combined dictionary\n",
    "    combined_dict.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of dictionaries into a more accessible format\n",
    "uptime_dict = {}\n",
    "for d in combined_dict:\n",
    "    for server_id, uptime_ranges in d.items():\n",
    "        uptime_dict[server_id] = uptime_ranges\n",
    "#uptime_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if the timestamp is within the server's uptime ranges\n",
    "def is_probe_up(probe_id, timestamp):\n",
    "    if probe_id in uptime_dict:\n",
    "        for uptime_range in uptime_dict[probe_id]:\n",
    "            if uptime_range['to'] is None:\n",
    "                if uptime_range['from'] <= timestamp:\n",
    "                    return \"connected\"\n",
    "            else:\n",
    "                if uptime_range['from'] <= timestamp <= uptime_range['to']:\n",
    "                    return \"connected\"\n",
    "    return \"disconnected\"\n",
    "\n",
    "# Iterate through the DataFrame and check if the timestamp occurred during the server's uptime\n",
    "df['probe_status'] = df.apply(lambda row: is_probe_up(row['prb_id'], row['timestamp']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking how mean and standard deviation vary over time \n",
    "\n",
    "grouped_data = df.groupby(['prb_id', 'dst_id'])\n",
    "mean = grouped_data['avg_rtt'].mean()\n",
    "std = grouped_data['avg_rtt'].std()\n",
    "distance = grouped_data['distance'].unique()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.scatter(distance, mean, label='Mean',color=\"BLUE\")\n",
    "plt.ylabel('Mean')\n",
    "plt.legend()\n",
    "plt.title('Mean and Standard Deviation vs. Distance')\n",
    "\n",
    "fig2, ax2 = plt.subplots()\n",
    "plt.errorbar(distance, mean, yerr=std, fmt='o',label='Standard Deviation',color=\"GREEN\")\n",
    "\n",
    "plt.xlabel('Distance')\n",
    "plt.ylabel('Standard Deviation')\n",
    "plt.legend()\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.tick_params(axis='x', which='both', bottom=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying some simple forecasting methods\n",
    "\n",
    "# Using the naive forecast\n",
    "df = df.assign(naive=df['rtt'].shift(1))\n",
    "# Replace NaN at top of value column with 0\n",
    "df['naive'] = df['naive'].fillna(method='ffill').fillna(0)\n",
    "\n",
    "# Testing the prediction accuracy for naive forecast\n",
    "se = (df['rtt'] - df['naive']) ** 2\n",
    "9 mse_naive = se.mean()\n",
    "10 mse_naive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponential smoothing method\n",
    "from statsmodels.tsa.api import SimpleExpSmoothing\n",
    "fit1 = SimpleExpSmoothing(df_sktime['avg']).fit()\n",
    "df['Simple-smoothing'] = SimpleExpSmoothing(df['avg']).fit().fittedvalues\n",
    "df[['avg','Simple-smoothing']].plot(title='Exponential Smoothing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the decision tree \n",
    "# randomising the test and train data \n",
    "import itertools\n",
    "import random\n",
    "\n",
    "test_indices = []\n",
    "train_indices = []\n",
    "        \n",
    "array1 = df2['nprb_id'].unique()\n",
    "array2 = df2['dst_addr'].unique()\n",
    "\n",
    "# Creating all possible pairs\n",
    "pairs = list(itertools.product(array1, array2))\n",
    "\n",
    "# Randomly selecting 10 pairs\n",
    "selected_pairs = random.sample(pairs, 10)\n",
    "\n",
    "# Removing selected pairs from the original list\n",
    "for pair in selected_pairs:\n",
    "    pairs.remove(pair)\n",
    "\n",
    "# Creating separate lists\n",
    "selected_list = selected_pairs\n",
    "remaining_list = pairs\n",
    "\n",
    "train_dfs = []\n",
    "for i,k in remaining_list:\n",
    "    temp_df = df2.loc[(df2['nprb_id'] == i) & (df2['dst_addr'] == k)]\n",
    "            \n",
    "    # Append the piece to the selected data\n",
    "    train_dfs.append(temp_df)\n",
    "\n",
    "train_df = pd.concat(train_dfs)\n",
    "        \n",
    "test_dfs = []\n",
    "for i,k in selected_list:\n",
    "    temp_df = df2.loc[(df2['nprb_id'] == i) & (df2['dst_addr'] == k)]\n",
    "            \n",
    "    # Append the piece to the selected data\n",
    "    test_dfs.append(temp_df)\n",
    "\n",
    "test_df = pd.concat(train_dfs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select your features and target\n",
    "X_train = train_df['normalizzed_distance'].values.reshape(-1,1)\n",
    "y_train = train_df['normalizzed_avg'].values\n",
    "\n",
    "X_test = test_df['normalizzed_distance'].values.reshape(-1,1)\n",
    "y_test = test_df['normalizzed_avg'].values\n",
    "\n",
    "# Import the Machine learning libraries\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Create a Decision Tree Regressor\n",
    "model = DecisionTreeRegressor()\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "# Make predictions on the testing set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the Decision Tree\n",
    "from sklearn.tree import export_graphviz\n",
    "tree_dot = export_graphviz(reg_tree,feature_names =[\"distance\"],out_file=None,rounded=True, filled=True)\n",
    "\n",
    "# Visualize the tree using Graphviz\n",
    "graph = graphviz.Source(tree_dot)\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Modelling \n",
    "import tensorflow as tf\n",
    "from tensorflow. keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', input_shape=(steps,features)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "model.fit(Xtrain, y_train, epochs=20, verbose=0)\n",
    "y_pred = model.predict(Xtest)\n",
    "\n",
    "mse = mean_squared_error(y_pred,y_test[0:len(y_pred)])\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D ) LATENCY Vs DISTANCE \n",
    "Sample code below helps us see how latency would change depending on the distance and when/if you added servers to deliver the customer services "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# References \n",
    "\n",
    "# Install Jupyter-MATLAB\n",
    "# https://am111.readthedocs.io/en/latest/jmatlab_install.html\n",
    "# Calling user-defined MATLAB functions from Python\n",
    "# https://www.mathworks.com/help/matlab/matlab_external/call-user-script-and-function-from-python.html\n",
    "\n",
    "\n",
    "# Check if python version is 64bit or 32bit\n",
    "# Then download the corresponding MATLAB version \n",
    "# import sys\n",
    "# print(sys.maxsize > 2**32)\n",
    "\n",
    "# MATLAB-side configuration\n",
    "# First Install MATLAB from this website https://www.mathworks.com/\n",
    "# Get the matlab root directory by running <matlabroot> in the MATLAB command window\n",
    "# Add the matlabroot/bin to the system path\n",
    "# export PATH=\"/Applications/MATLAB_R2019b.app/bin:$PATH\"\n",
    "\n",
    "# SETUP MATLAB ENGINE API FOR PYTHON\n",
    "# cd /usr/local/MATLAB/R2018a/extern/engines/python -  change to your matlab version\n",
    "# python setup.py install // change setup tools if this fails pip install setuptools==58.2.0\n",
    "\n",
    "# JUPYTER SIDE CONFIGURATION \n",
    "# python -m matlab_kernel install --user //this adds matlab to the jupyter kernels list\n",
    "# jupyter kernelspec list //check if matlab is in the list\n",
    "# pip install matlab.engine\n",
    "\n",
    "import matlab.engine\n",
    "\n",
    "# Start a MATLAB session\n",
    "eng = matlab.engine.start_matlab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call the matlab simulation \n",
    "eng.PoA_student_workshop(nargout=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
