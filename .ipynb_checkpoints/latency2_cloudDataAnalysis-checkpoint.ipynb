{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Cloud through data analysis\n",
    "This project is accesible at: https://github.com/rita-imdea/ripe-userguide \n",
    "The supplementary PDF file can be found at https://github.com/rita-imdea/ripe-userguide/blob/main/latency2_cloudDataAnalysis_GUIDE_student.pdf\n",
    "\n",
    "In this notebook, we are going to see how to retrieve measurements fron RIPE Atlas and also how to parse measurements from JSON files (which have been pre-downloaded from RIPE Atlas). \n",
    "We recommend the reader to first take a look at the PDF document that contains the guide for the whole course. \n",
    "\n",
    "------------------------------\n",
    "\n",
    "This notebook is divided in 2 parts. \n",
    "\n",
    "    \n",
    "#### A) PARSING YOUR MEASUREMENTS\n",
    "    We will explain how to clean and pre-process the data, which are *essential* steps to obtain good results.\n",
    "    \n",
    "#### B) ANALYZING YOUR MEASUREMENTS \n",
    "    We show how important is to understand the data before drawing some conclusions. After that, we apply an statistical analysis, where we will see both simple statistical models and machine learning models based on training.\n",
    "    \n",
    "  \n",
    "------------------------------\n",
    "\n",
    "This project makes use of data from RIPE Atlas, a measurement platform that contains public measurements. These measurements consists of the parameters and statististics of all the messages sent from some source devices, named \"probes\", towards some IP addresses or urls, named \"destinations\". These data allow us to understand how Internet works and how time and space impact on latency. \n",
    "\n",
    "Let's start!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "------------------------------\n",
    "\n",
    "First of all, we need to install some python packages that are required to contact with the RIPE Atlas server and to parse JSON files. Some other packages will be imported later on when they are needed. Make sure that you have installed the packages (see the guide that is accessible from the link on top of this file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # to create http requests from Python\n",
    "import json     # Library to write and ready JSON files in Python\n",
    "from matplotlib import pyplot as plt # To plot the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we check how to pass from a retrieved JSON file to a meaningful format in Python to work with the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) PARSING YOUR MEASUREMENTS\n",
    "\n",
    "We are going to get the data from the JSON files, which are already present in the course directory; you can also obtain other measurements from https://atlas.ripe.net/measurements/, as explained in https://github.com/rita-imdea/ripe-userguide/blob/main/latency1_whereIsTheCloud_B_getMeasurement.ipynb\n",
    "\n",
    "The next steps are:\n",
    "\n",
    "#### a) Initially, we parse the JSON files and store the data in a Python structure \n",
    "#### b) We clean the data, add new information, remove useless fields, and transform the data to provide the correct format to the algorithms. \n",
    "\n",
    "-----------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we set the IDs of the measurements we are interested in. \n",
    "In this case, the IDs provided below refer to the measurements taken for this course. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#measurement_ids = [\"48819905\", \"48819907\", \"48819909\"] # example of measurement ID -- PING MEASUREMENTS\n",
    "#measurement_ids = [\"48819906\", \"48819908\", \"48819910\"] # example of measurement ID -- TRACEROUTE MEASUREMENTS\n",
    "measurement_ids = [\"48819905\",  \"48819907\",  \"48819909\"] # example of measurement ID\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Reading JSON files in Python\n",
    "\n",
    "Next, we use the Python library \"json\", which allows us to ready and write JSON files with Python.\n",
    "Furthermore, we use two of the most used packages for data processing in PYthon: Pandas and Numpy. \n",
    "\n",
    "1. Pandas is a Python library that provides a data structure called a DataFrame (https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html), which is a structure that facilitates data processing and manipulation. We can visualize it as a matrix that contains data of different format. We shall use it to parse our data because it provides a number of useful functions for manipulation and visualization of data.\n",
    "\n",
    "2. Numpy is a Python library that facilitates mathematical operations, in particular for arrays and matrices. \n",
    "\n",
    "3. We also import datetime to work with different time formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first import the necessary libraries \n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each one of the measurements, we read the JSON file and store it in a pandas DataFrame (df) structure.\n",
    "\n",
    "Then, we create a list of DataFrames (df) with all the measurements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the measurement IDs experiment files and create a DataFrame for each\n",
    "\n",
    "dfs = [] # creating an empty vector for DataFrame elements\n",
    "\n",
    "for measurement_id in measurement_ids: # For each of the measurements that we are considering:\n",
    "    \n",
    "    # Read the JSON data from the file\n",
    "    with open(f\"RIPE-Atlas-measurement-{measurement_id}.json\", \"r\") as f:\n",
    "        json_data = json.load(f)\n",
    "    \n",
    "    # Normalize the JSON data into a pandas DataFrame\n",
    "    df = pd.json_normalize(json_data)\n",
    "    \n",
    "    # Append the DataFrame to the list of DataFrames\n",
    "    dfs.append(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to have all the DataFrames together to analyze all the measurements at the same time. Thus, we concatenate the list of dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all the DataFrames into a single one\n",
    "result_df = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the current data structure thanks to the pandas function \"head\", which prints the first 5 rows of the DataFrame with the value of all the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the resulting DataFrame\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Data cleaning and pre-processing\n",
    "\n",
    "The following step is to clean the data collection. \n",
    "\n",
    "For that, we should first know that the RIPE Atlas measurements may include negative values for latency (-1.0), which represent samples for which it was not possible to store the value (Due to, for example, timeouts in the network protocols). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid that these values impact the later analysis, we remove those samples. We first transform the negative values into Not-a-Number values (NaN), and we make use of the function \"dropna\" from Pandas library to remove those samples.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaning the data \n",
    "\n",
    "original_data_size = result_df.shape # saving the number of rows and columns for later\n",
    "\n",
    "# Change the negative values to NaN\n",
    "result_df['avg'].replace(-1.0, np.nan, inplace=True)\n",
    "\n",
    "# Remove NaN values \n",
    "result_df = result_df.dropna(how='any',axis=0) \n",
    "\n",
    "print(f\"Original size: {original_data_size} /// Processed size = {result_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily check the percentage of \"broken\" samples that there were:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(f\"We have {original_data_size[0]-result_df.shape[0]} bad samples {original_data_size[0]} among total samples.\")\n",
    "\n",
    "print(f\"That means that {(original_data_size[0]-result_df.shape[0])/original_data_size[0]*100:.2f}% of the samples have been discarded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We further pre-process the data to facilitate the plotting, visualization and readability of the data.\n",
    "\n",
    "For that, we change the probe ID with an acronym that represents the country where the probe is located, plus an index in case there are several probes in the same country. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the probe_id column for easy plotting \n",
    "nprb_id = []\n",
    "\n",
    "for value in result_df[\"prb_id\"]:\n",
    "    if value == 1004991:\n",
    "        nprb_id.append('es1')\n",
    "    if value == 53229:\n",
    "        nprb_id.append('es2')\n",
    "    if value == 1004997:\n",
    "        nprb_id.append(\"pt1\")\n",
    "    if value == 1004102:\n",
    "        nprb_id.append(\"pt2\")\n",
    "    if value == 20757:\n",
    "        nprb_id.append(\"es3\")\n",
    "    if value == 1003454:\n",
    "        nprb_id.append(\"nl1\")\n",
    "    if value == 1003158:\n",
    "        nprb_id.append(\"nl2\")\n",
    "    if value == 1003747:\n",
    "        nprb_id.append(\"nl3\")\n",
    "    if value == 1004200:\n",
    "        nprb_id.append(\"es4\")\n",
    "    if value == 51381:\n",
    "        nprb_id.append(\"ug1\")\n",
    "    if value == 54470:\n",
    "        nprb_id.append(\"us1\")\n",
    "    if value == 1002914:\n",
    "        nprb_id.append(\"us2\")\n",
    "        \n",
    "# Saving the new IDs in the column \"nprb_id\"\n",
    "result_df[\"nprb_id\"] = nprb_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we edit the format in which the time is saved. By default, the time units provided by RIPE Atlas are UNIX timestamps, i.e., the number of seconds passed since 1st January 1970 at 00:00h."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Changing the time column from epoch to date time format for time series processing\n",
    "\n",
    "new_timestamp = [] # initializing the new array\n",
    "\n",
    "# populating the new array\n",
    "for i in result_df['timestamp']:\n",
    "    my_datetime = datetime.fromtimestamp(i) # transforming the timestamp in standard time\n",
    "    new_timestamp.append(my_datetime) # adding the value to the array\n",
    "\n",
    "# Saving the new time format in the column \"new_time\"\n",
    "result_df['new_time'] = new_timestamp\n",
    "result_df['new_time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last pre-processing is to save the code of the country in which the probe is located. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# itializing the country code array\n",
    "country_name = []\n",
    "\n",
    "for value in result_df[\"nprb_id\"]:\n",
    "    if (value == 'es1') or (value == 'es2') or(value == 'es3') or( value == 'es4'):\n",
    "        country_name.append('Spain')\n",
    "    if(value == 'nl1') or (value == 'nl2')or (value == 'nl3'):\n",
    "        country_name.append('Netherlands')\n",
    "    if(value == 'pt1') or (value == 'pt2'):\n",
    "        country_name.append('Portugal')\n",
    "    if(value == 'us1') or (value == 'us2'):\n",
    "        country_name.append('USA')\n",
    "    if(value == 'ug1'):\n",
    "        country_name.append('Uganda')\n",
    "\n",
    "# Saving the new country code in the column \"country_name\"\n",
    "result_df['country_name'] = country_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize if the new additions are correct by calling again the method df.head(). You will see that the last three columns coincide with the content that we have just added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "\n",
    "We have already cleaned and structured our data. Now, it's time to analyze and extract valuable information from it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "------------------------------\n",
    "------------------------------\n",
    "\n",
    "### B) ANALYZING YOUR MEASUREMENTS \n",
    "\n",
    "We split this section also in two parts. \n",
    "\n",
    "a) Check how latency varies over time, how mean and standard deviation vary over distance or any other interesting scenarios you can come up with \n",
    "\n",
    "b) Finally we can do some predictions based using established mathematical models or machine learning models and see what gives us the best results.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Understanding the data\n",
    "The first step is to take a look at the data we have and try to obtain insights that can help us with the later analysis.\n",
    "\n",
    "Let us first check what is the probability distribution of the latency for the whole dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For that, we plot the discrete PDF based on the histogram of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the discrete PDF from the histogram\n",
    "\n",
    "axx = result_df['avg'].hist(density=True, bins=100, alpha = 0.8) \n",
    "# bins = Number of points in the histogram.\n",
    "# alpha = a parameter for color transparency\n",
    "\n",
    "# We can add information into the graph\n",
    "axx.set_ylabel(\"Probability Density Function\")\n",
    "axx.set_xlabel(\"Latency (ms)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We realize that the shape is quite curious, with several peaks, looking like a decreasing wave. \n",
    "\n",
    "**What can be the reason?**<br>\n",
    "\n",
    "**Please, take a moment to think about it and comment in group what you think about the graph above**<br> \n",
    "\n",
    "Don't continue further down until you took your time to talk about it.\n",
    "\n",
    "---------------------------------------------------------------\n",
    "\n",
    "**... \n",
    "<br><br>\n",
    "...\n",
    "<br><br>\n",
    "...\n",
    "<br><br>\n",
    "...\n",
    "<br><br>\n",
    "...\n",
    "<br><br>\n",
    "...\n",
    "<br><br>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have you already thought about it? Let us see if we can dive a bit deeper into the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider now some of the information we have to try to get more insights.<br>\n",
    "\n",
    "First, let us check what is the PDF for the data originated in each probe, separately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking PDF for each of the probes\n",
    "\n",
    "# First, we recover the array of probe IDs. These IDs are identifiers used for RIPE Atlas, the measurements platform. \n",
    "probes = [1004991,53229,20757,1004200,1003454,1003747,1004102,51381,54470,1002914]\n",
    "         \n",
    "# We extract the data corresponding to each probe, and plot it separately but in the same plot\n",
    "for probe in probes:\n",
    "    df_cdf = result_df[(result_df['prb_id'] == probe)]  # Get the data that corresponds to one probe\n",
    "    axx = df_cdf['avg'].hist(density=True, bins=100, alpha = 0.3) # Draw the histogram\n",
    "\n",
    "# We add the probe IDs as legend\n",
    "axx.legend(probes)\n",
    "axx.set_xlabel(\"Latency (ms)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we observe?<br>\n",
    "\n",
    "**Any ideas??**<br>\n",
    "\n",
    "\n",
    "Take a moment to propose ideas before continuing. \n",
    "<br>\n",
    "\n",
    "---------------------------------------------------------------\n",
    "\n",
    "**... \n",
    "<br><br>\n",
    "...\n",
    "<br><br>\n",
    "...\n",
    "<br><br>\n",
    "...\n",
    "<br><br>\n",
    "...\n",
    "<br><br>\n",
    "...\n",
    "<br><br>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(...)\n",
    "\n",
    "\n",
    " \n",
    "<br><br>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it seems that there are some probes that are easily differentiable from the rest of the probes.\n",
    "But that plot shows us little information about **why**.\n",
    "\n",
    "Also, the probe IDs is not a very meaningful value. We can use the attibute that we have previously added identifying probes with the country where they are located. Let us know plot the same graph with the new ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking PDF for each of the probes\n",
    "\n",
    "# First, we recover the array of probe IDs\n",
    "probes = ['es1','es2','es3','es4','nl1','nl3','pt2','ug1','us1','us2']\n",
    "         \n",
    "# We extract the data corresponding to each probe, and plot it separately but in the same plot\n",
    "for probe in probes:\n",
    "    df_cdf = result_df[(result_df['nprb_id'] == probe)] # Get the data that corresponds to one probe\n",
    "    axx = df_cdf['avg'].hist(density=True, bins=100, alpha = 0.3) # Draw the histogram\n",
    "\n",
    "# We add the probe IDs as legend\n",
    "axx.legend(probes)\n",
    "axx.set_xlabel(\"Latency (ms)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that the country of location have an important impact. In order to better verify it, we can show the results aggregated by country. Let us see what is the outcome of this visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Obtaining PDF per country\n",
    "\n",
    "# Retrieving list of countries from the data\n",
    "countries = result_df['country_name'].unique()\n",
    "\n",
    "for country in countries:\n",
    "    df_pdf = result_df[(result_df['country_name'] == country)] # Get the data that corresponds to one country\n",
    "    axx = df_pdf['avg'].hist(density=True, bins=100, alpha = 0.3) # Draw the histogram \n",
    "\n",
    "axx.legend(countries)\n",
    "axx.set_xlabel(\"Latency (ms)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see more clearly that **location matters** for latency and internet. This, which is expected by all of us, it is not necessary true if we do not corroborate it with data. \n",
    "\n",
    "More in detail, Portugal and Spain obtain the same performance, as they are next to each other and the servers are located in further places such us Netherlads, U.S., and other countries. \n",
    "\n",
    "This results showcases the fact that, if we want to know whether some edge-cutting application can be deployed in a certain place, we need to first make sure that the network has a sufficiently good performance to actually allow for the service to work.\n",
    "\n",
    "In the following, we advance a bit deeper on the characterization of the data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One crucial value to understand the latency values is to know where each of the probes and destinations are located. \n",
    "\n",
    "For that, we now obtain the coordinates of all the devices (sources and destinations) participating in the experiment, and we include it in the DataFrame object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Obtaining geographical information of the probes \n",
    "\n",
    "# Import the package needed to format the metadata of RIPE's probes.\n",
    "from ripe.atlas.cousteau import Probe \n",
    "\n",
    "#Getting all the ripe atlas probes\n",
    "probe_id_list = result_df['prb_id'].unique()\n",
    "probe_nid_list = result_df['nprb_id'].unique()\n",
    "\n",
    "# Collect the source probe latitude/longitude information from ripe atlas \n",
    "probe_coordinates = []\n",
    "probe_country = []\n",
    "\n",
    "for id_i in probe_id_list: # for each one of the probes:\n",
    "    probe = Probe(id=id_i) # Obtains all metadata of probe id_i\n",
    "    #print(probe.geometry) #probe.geometry is a GeoJSON https://en.wikipedia.org/wiki/GeoJSON\n",
    "    probe_coordinates.append(probe.geometry['coordinates']) # saving the coordinates to the list of coordinates\n",
    "    probe_country.append(probe.country_code)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a list with the information of coordinates for each of the probes. Next, we tranform it into latitude and longitude, and we add the country in which they are located. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longitude = []\n",
    "latitude = []\n",
    "\n",
    "for i in probe_coordinates:\n",
    "    longitude.append(i[0])\n",
    "    latitude.append(i[1])\n",
    "\n",
    "# create a probe metadata dataframe\n",
    "srcprobes_df = pd.DataFrame({'prb_id': probe_nid_list,'longitude': longitude, 'latitude': latitude,'probe_country': probe_country})\n",
    "srcprobes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have done this for the probes (source nodes). We can repeat the procedure for the destination addresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Obtaining geographical information of the destinations \n",
    "\n",
    "#Getting all the destination ip addresses\n",
    "dest_ip_list = result_df['dst_addr'].unique()\n",
    "\n",
    "# Create empty lists to store the data\n",
    "server_ip = []\n",
    "latitudes = []\n",
    "longitudes = []\n",
    "\n",
    "# Query the \"ipinfo.io\" API for geolocation information\n",
    "for ip in dest_ip_list:\n",
    "    response = requests.get(f'https://ipinfo.io/{ip}/json')\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        latitude, longitude = data.get('loc', '').split(',')\n",
    "        server_ip.append(ip)\n",
    "        latitudes.append(latitude)\n",
    "        longitudes.append(longitude)\n",
    "\n",
    "# Create a dataframe with the geophraphical information of the destinatiuon addresses. Remember that this infomation is approximate for privacy reasons\n",
    "a_server_df = pd.DataFrame({ 'Server_ip': server_ip,'latitude': latitudes, 'longitude': longitudes})\n",
    "a_server_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now all the information regarding the location of the addresses participating in the experiment. Thus, we can now calculate the distance between sources and destinations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Obtaining the distance between nodes. \n",
    "\n",
    "# We first define the function that computes the Euclidean distance between two points\n",
    "distance_data = []\n",
    "def euclidean_distance(lat1, lon1, lat2, lon2):\n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    return np.sqrt((lat1 - lat2)**2 + (lon1 - lon2)**2)\n",
    "\n",
    "# Create an empty dataframe to store distance information\n",
    "distance_df = pd.DataFrame(columns=['prb_id', 'ip_address', 'distance'])\n",
    "\n",
    "# Calculate Euclidean distances between all probes and destination addresses\n",
    "for index_probe, row_probe in srcprobes_df.iterrows():\n",
    "    for index_dst, row_dst in a_server_df.iterrows():\n",
    "        distance = euclidean_distance(float(row_probe['latitude']), float(row_probe['longitude']), float(row_dst['latitude']), float(row_dst['longitude']))\n",
    "        distance_data.append({\n",
    "            'nprb_id': row_probe['prb_id'],\n",
    "            'dst_addr': row_dst['Server_ip'],\n",
    "            'distance': distance})\n",
    "        \n",
    "# Create a DataFrame from the list of distance information\n",
    "distance_df = pd.DataFrame(distance_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can show the distance between all probes and destinations as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now add this information to our general DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the distance data to the result_df.\n",
    "# For that, for each of the points that have the same ['nprb_id', 'dst_addr'] tuple, we add the field of distance between those nodes. \n",
    "analysis_df  = pd.merge(result_df,distance_df, left_on=['nprb_id', 'dst_addr'], right_on=['nprb_id', 'dst_addr'], how='inner')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the first 5 rows to check that the value is there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------\n",
    "\n",
    "### b) Predicting the data\n",
    "\n",
    "Now it is turn to see whether we can actually predict the latency on Internet!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to start with very simple forecasting methods, to observe if the data is easily predictable or we need more complex algorithms. After that, we will check also more involved machine learning algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------\n",
    "\n",
    "#### Simple methods\n",
    "\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start evaluating simple and classical methods that offer an efficient (but limited) manner of predicting the next values. \n",
    "These methods are: the *Naive forecast* and *Exponential Smoothing*. \n",
    "\n",
    "We aim at predicting the next values of the time series of average latency. The column in our dataset representing the average latency is 'avg'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But... **there is a very important step to apply before predicting!!!**\n",
    "\n",
    "These methods take the previous N samples to estimate the next one. For that, we need to structure or data such that it follows a strict time order. Note that if we check now our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df[['src_addr', 'dst_addr', 'new_time']].head(15) # Print first 15 rows of some columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***If we try to estimate the future sample of one link with the previous samples, we are using samples from other links!***\n",
    "We need to correctly order the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We sort the values by link ('src_addr', 'dst_addr') and time ('timestamp')\n",
    "sorted_df = result_df.sort_values(by=['src_addr', 'dst_addr', 'timestamp'], inplace = False) # inplace=False == New object\n",
    "sorted_df.reset_index(drop=True, inplace = True) # We reset the index of the dataframe to avoid issues.\n",
    "sorted_df[['src_addr', 'dst_addr', 'new_time']].head(15) # Print first 15 rows of some columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now evaluate the two methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first one is as simple as \"Naive forecast\", which consists on just considering that the next sample is going to be exactly the same as the previous one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying some simple forecasting methods\n",
    "\n",
    "# Using the naive forecast\n",
    "sorted_df['naive'] = sorted_df['avg'].shift(1) # next sample is the previous one\n",
    "sorted_df[['avg', 'naive']].head(7) # Print first rows of some columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the first value do not have any forecast. To avoid problems with NaN, we assign the exact value for the first sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN at top of value \n",
    "sorted_df.at[0,'naive'] = sorted_df.at[0,'avg']\n",
    "sorted_df[['avg', 'naive']].head(7) # Print first  rows of 'avg' and 'naive'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate the Mean Squared Error of this algorithm as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the prediction accuracy for naive forecast\n",
    "se = (sorted_df['avg'] - sorted_df['naive']) ** 2\n",
    "mse_naive = se.mean()\n",
    "mse_naive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compare this simple algorithm with more involved ones. \n",
    "The next one is Exponential smoothing. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exponential smoothing** is a technique qhere the next sample is obtained as a weigthed moving average of previous samples. The weights are selected such that they decrease exponentially. \n",
    "The simplest form of exponential smoothing is given by the following expressions, where y_t is the prediction at time t for the future sample and x_t is the actual value at time t.\n",
    "\n",
    "$y_0 = x_0$\n",
    "\n",
    "$y_t = \\alpha x_t + (1-\\alpha)y_{t-1}$ for $t>0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate this algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exponential smoothing method\n",
    "\n",
    "# import the library that allows us to easily apply it\n",
    "from statsmodels.tsa.api import SimpleExpSmoothing\n",
    "\n",
    "# Fitting (adjusting) the data to an exponential smoothing approach. \n",
    "fit1 = SimpleExpSmoothing(sorted_df['avg']).fit()\n",
    "\n",
    "# Evaluating the result of the fitting\n",
    "sorted_df['simple_smoothing'] = SimpleExpSmoothing(sorted_df['avg']).fit().fittedvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the Naive forecast, we can evaluate the Mean Squared Error of this algorithm as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se = (sorted_df['avg'] - sorted_df['simple_smoothing']) ** 2\n",
    "mse_esm = se.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can compare them:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MSE Naive: {mse_naive:.2f}, MSE Exponential smoothing: {mse_esm:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exponential smoothing already improves w.r.t. the naive method.\n",
    "\n",
    "We can plot the results of both methods as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results\n",
    "samp_start_plot = 300 # last sample showed\n",
    "samp_end_plot   = 700 # fir sample showed\n",
    "\n",
    "plt.plot(sorted_df['avg'].iloc[samp_start_plot:samp_end_plot],  linewidth = 2, color = 'black')\n",
    "plt.plot(sorted_df['naive'].iloc[samp_start_plot:samp_end_plot], '.', color='green', linewidth=1)\n",
    "plt.plot(sorted_df['simple_smoothing'].iloc[samp_start_plot:samp_end_plot], '.', color = 'blue', linewidth=1)\n",
    "plt.title('Simple methods')\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10, 7)\n",
    "fig.set_dpi(200)\n",
    "\n",
    "plt.legend(['Real latency', 'Naive', 'Exponential smoothing'])\n",
    "plt.xlabel(\"samples\")\n",
    "plt.ylabel(\"latency (ms)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how the naive method is more sensitive to noise and the exponential nature of the smoothing can be observed in the peaks: the following samples depend on the peak/outlier value in a way that decreases exponentially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we move to evaluating machine learning algorithms. \n",
    "\n",
    "We are going to begin with a simple problem that aims at predicting the distance based on the latency between source and destination. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------\n",
    "\n",
    "#### Machine learning methods   \n",
    "\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to separate the data in training and test set. The training set is the data we will use to train the model and let it learn. Then, we evaluate the performance in the test data. This approach is needed in order to avoid that the method \"overfit\", that is, that it adapts too much to the data it sees but it is not able to adapt to new data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import some useful packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using machine learning models\n",
    "\n",
    "# We import the necessary packages. These ones are already included in the Python distribution, so you don't need to install them\n",
    "import itertools\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split the data as explained. For that, we split the data based on the source-destination pair. We select 70% of the pairs as training data and the remaining 30% as test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First we randomize the dataset and then split it into training and testing sets before applying the models\n",
    "df2 = analysis_df.copy() # Making a copy of the data\n",
    "\n",
    "## First of all, we select a subset of source-destination pairs for training and the rest for testing the training.\n",
    "test_indices  = []\n",
    "train_indices = []\n",
    "        \n",
    "# We obtain the identifiers for source and destination\n",
    "array1 = df2['nprb_id'].unique()\n",
    "array2 = df2['dst_addr'].unique()\n",
    "\n",
    "# Creating all possible pairs of source - destination\n",
    "pairs = list(itertools.product(array1, array2))\n",
    "\n",
    "# Randomly selecting the 30% of those pairs for testing\n",
    "num_test_pairs = int(len(pairs)/3)\n",
    "test_pairs     = random.sample(pairs, num_test_pairs)\n",
    "\n",
    "# Creating separate lists\n",
    "test_list  = test_pairs\n",
    "training_list = pairs.copy()\n",
    "\n",
    "# Removing selected pairs from the original list\n",
    "for pair in test_list:\n",
    "    training_list.remove(pair)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have randomly selected the pairs, we create two different dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieving training data\n",
    "train_dfs = []\n",
    "for i, k in training_list:\n",
    "    # Get the data from the dataframe corresponding to this given pair\n",
    "    temp_df = df2.loc[(df2['nprb_id'] == i) & (df2['dst_addr'] == k)]\n",
    "            \n",
    "    # Append the pair data to the selected data\n",
    "    train_dfs.append(temp_df)\n",
    "\n",
    "# Concat all training pairs' data\n",
    "train_df = pd.concat(train_dfs)\n",
    "  \n",
    "## Retrieving test data\n",
    "\n",
    "test_dfs = []\n",
    "for i, k in test_list:\n",
    "    # Get the data from the dataframe corresponding to this given pair\n",
    "    temp_df = df2.loc[(df2['nprb_id'] == i) & (df2['dst_addr'] == k)]\n",
    "            \n",
    "    # Append the pair data to the selected data\n",
    "    test_dfs.append(temp_df)\n",
    "\n",
    "test_df = pd.concat(test_dfs)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to do a very important step: the normalization of input data. This step is very important because it impacts considerably in the performance of the algorithms. A correct normalization of the input data may make a previously useless approach into the most accurate one, since the algorithms are very sensitive to the range and variation of the data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normalization we use is such that we obtain a variable whose mean is zero and variance is 1. \n",
    "Note that, since we can only train the method with the training data, we consider the training mean and training variance as the correct values, although that will create a (usually small) error with respect to the real mean and variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalization of input data\n",
    "\n",
    "#normalize the latency values \n",
    "train_df['normalized_latency'] = (train_df['avg'] - train_df['avg'].mean()) / train_df['avg'].std()\n",
    "test_df['normalized_latency']  = (test_df['avg']  - train_df['avg'].mean()) / train_df['avg'].std()\n",
    "\n",
    "# normalize the 'distance' column\n",
    "train_df['normalized_distance'] = (train_df['distance'] - train_df['distance'].mean()) / train_df['distance'].std()\n",
    "test_df['normalized_distance']  = (test_df['distance']  - train_df['distance'].mean()) / train_df['distance'].std()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the input data is correctly structured, we can start evaluating different algorithms. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start evaluating a <b>Decision Tree<b>.\n",
    "    \n",
    "For that, we first need to import the required libraries that allow us to implement the decision tree learning without re-implementing it again. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the Machine learning libraries related to Decision trees\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we obtain the input data and the \"labels\" (correct output values) for train and test. Generally, X represents the input and Y the output. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select your features and target\n",
    "X_train = train_df['normalized_latency'].values.reshape(-1,1)\n",
    "y_train = train_df['normalized_distance'].values\n",
    "\n",
    "X_test = test_df['normalized_latency'].values.reshape(-1,1)\n",
    "y_test = test_df['normalized_distance'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train the decision tree. Since the value we want to predict (latency) is continuous, we need a regressor.\n",
    "In the next lines, we fit the model to our training data, which consists on optimizing the model parameters to reduce the error of the prediction for the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a Decision Tree Regressor\n",
    "model = DecisionTreeRegressor()\n",
    "## Train the model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, it is very simple with the available libraries. \n",
    "We can also verify the performance of this decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make predictions on the testing set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "mse_dt = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error of the decision tree: {mse_dt:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the MSE above is normalized because the inputs are normalized. We can denormalize it to obtain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean Squared Error of the decision tree: {mse_dt*train_df['distance'].std():.2f} km\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a bit difficult to understand what is going on behind the code above. \n",
    "\n",
    "<b>We can also visualize the tree!<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the Decision Tree\n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "\n",
    "## Generating the scheme of the tree based on the distance, with some extra parameters\n",
    "tree_dot = export_graphviz(model,feature_names =[\"latency\"],out_file=None, rounded=True, filled=True)\n",
    "\n",
    "# Visualize the tree using Graphviz\n",
    "graph = graphviz.Source(tree_dot)\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will probably appear too big for the window. Don't worry, you can save the image to better understand it. \n",
    "It is quite simple: The model consists of a series of True/False questions, each \"leaf\" (end node) leads you to a predicted value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After seeing the decision tree solution, we start evaluating a <b>LSTM Model<b>.\n",
    "\n",
    "Long short-term memory networks are recurrent neural networks (RNN) that are a very good fit to time-series forecasting, as their output depends on past decisions.\n",
    "    \n",
    "For that, we also need to import the required libraries that allow us to implement the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Modelling \n",
    "# If you want to know more about RNN & LSTM, you can follow this 20 minutes tutorial: https://www.youtube.com/watch?v=BSpXCRTOLJA\n",
    "import tensorflow as tf\n",
    "from tensorflow. keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "\n",
    "# We are going to use also one function from numpy package\n",
    "from numpy import array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, RNN receive as input <b> Sequences <b>. This means that we have to re-structure the data such that each one of the input sets are time-ordered sequences. For that, we define the following function.\n",
    "\n",
    "This function, \"split_sequence\", receives a time series and outputs the last \"n_steps\" samples for each one of the time samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating sequences of past samples. \n",
    "\n",
    "# n_steps = number of previous samples to consider\n",
    "\n",
    "# since we need n_steps previous samples, the prediction will start after the first n_steps time slots.\n",
    "\n",
    "def split_sequence(sequence, n_steps):\n",
    "    X = list()\n",
    "    for i in range(len(sequence)-n_steps):\n",
    "        seq_x = sequence[i:i + n_steps]\n",
    "        X.append(seq_x)\n",
    "    \n",
    "    # Important! Shuffling to help the method to learn different patterns\n",
    "    shuffled_X = np.array(X)\n",
    "    np.random.shuffle(shuffled_X)\n",
    "\n",
    "    return shuffled_X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the function defined, we can pass the train and test data through it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pastSamples = 30 # Number of past values that are taken into account for predicting. \n",
    "\n",
    "Xtrain_seq = split_sequence(X_train, n_pastSamples)\n",
    "Xtest_seq  = split_sequence(X_test,  n_pastSamples)\n",
    "\n",
    "Ytrain_seq = y_train[n_pastSamples:] # Because we can only predict from sample n_pastSamples\n",
    "Ytest_seq  = y_test[n_pastSamples:] # Because we can only predict from sample n_pastSamples\n",
    "\n",
    "print(f\"The new shape of our training data is: {Xtrain_seq.shape}, and the shape of the training labels is: {Ytrain_seq.shape}\")\n",
    "print(f\"The new shape of our test data is: {Xtest_seq.shape}, and the shape of the test labels is: {Ytest_seq.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is turn to define the neural network architecture and the learning parameters. \n",
    "\n",
    "Let us start with a simple architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define model\n",
    "n_features   = 1  # number of input features; in our case, only one: distance\n",
    "n_neurons    = 100 # number of neurons per layer\n",
    "\n",
    "model = Sequential() # Tipe of model\n",
    "\n",
    "# We add one LSTM layer. The activation function is the Rectified Linear function.\n",
    "# We also need to add the correct shape of the input values\n",
    "model.add(LSTM(n_neurons, activation='relu', input_shape=(n_pastSamples, n_features)))\n",
    "\n",
    "# We add an output layer consisting on a single neuron that outputs the predicted value.\n",
    "model.add(Dense(1))\n",
    "\n",
    "# We select the Mean Squared Error as the loss function, and we use the standard adam optimizer.\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to fit (train) the model!\n",
    "\n",
    "The parameter \"epochs\" denotes the number of \"rounds\" we pass through the training set. Let's train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fitting the model\n",
    "num_epochs = 20 # number of cycles of training through the training data\n",
    "\n",
    "# If you don't want to see the information about training, set verbose = 0\n",
    "model.fit(Xtrain_seq, Ytrain_seq, epochs=num_epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can predict the test labels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(Xtest_seq, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, of course, evaluate the MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_lstm = mean_squared_error(y_pred, Ytest_seq)\n",
    "\n",
    "print(f\"Mean Squared Error of the simple LSTM network: {mse_lstm:.2f} normalized MSE\")\n",
    "print(f\"Mean Squared Error of the simple LSTM network: {mse_lstm*train_df['distance'].std():.2f} km\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The internal parameters of the neural networks may matter a lot. The previous network is a very simple minimal LSTM network. Let's define more complex one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define model\n",
    "n_features   = 1   # number of input features; in our case, only one: distance\n",
    "n_neurons    = 100  # number of neurons per layer\n",
    "dropout_prob = 0.2 # Probability of dropping out neurons (it helps to generalize the training)\n",
    "\n",
    "model = Sequential() # Tipe of model\n",
    "\n",
    "# We add one LSTM layer. The activation function is the Rectified Linear function.\n",
    "# We also need to add the correct shape of the input values\n",
    "# return_sequences = True is needed if the following layer is LSTM because they require sequences\n",
    "model.add(LSTM(n_neurons, activation='relu', input_shape=(n_pastSamples, n_features), return_sequences = True))\n",
    "model.add(Dropout(dropout_prob)) # drop out of some values to improve generalization\n",
    "\n",
    "# We add one LSTM layer. The activation function is the Rectified Linear function.\n",
    "model.add(LSTM(n_neurons, activation='relu'))\n",
    "model.add(Dropout(dropout_prob)) # drop out of some values to improve generalization\n",
    "\n",
    "# We add a fully connected layer of 32 simple neurons. \n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(dropout_prob))\n",
    "\n",
    "# We add an output layer consisting on a single neuron that outputs the predicted value.\n",
    "model.add(Dense(1))\n",
    "\n",
    "# We select the Mean Squared Error as the loss function, and we use the standard adam optimizer.\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train this new model and check the performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fitting the model\n",
    "num_epochs = 20 # number of cycles of training through the training data\n",
    "\n",
    "# If you don't want to see the information about training, set verbose = 0\n",
    "model.fit(Xtrain_seq, Ytrain_seq, epochs=num_epochs, verbose=1)\n",
    "\n",
    "y_pred = model.predict(Xtest_seq, verbose=1)\n",
    "\n",
    "mse_lstm2 = mean_squared_error(y_pred, Ytest_seq)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean Squared Error of the simple network: {mse_lstm2:.2f} normalized MSE\")\n",
    "print(f\"Mean Squared Error of the decision tree: {mse_lstm2*train_df['distance'].std():.2f} km\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
